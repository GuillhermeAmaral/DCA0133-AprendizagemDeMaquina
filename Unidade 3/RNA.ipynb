{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementação de uma Rede Neural em Python\n",
    "Para o trabalho da 3ª unidade do curso de Inteligência Artificial iremos construir classe que modela uma rede neural artificical que deve conter as seguintes características:\n",
    "* Quantidade de camadas e neurônios personalizáveis\n",
    "* Mais de uma função de ativação\n",
    "* Mais de uma função custo\n",
    "* Tamanho do lote (mini_batch) personalizável\n",
    "* Calcular pesos pelo algoritmo Backpropagation\n",
    "\n",
    "Neste trabalho iremos usar as seguintes bibliotecas do python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E precisaremos das funções abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''Função sigmoid'''\n",
    "    return 1.0/(1.0+np.exp(-x))\n",
    "\n",
    "def dsigmoid(x):\n",
    "    '''Derivada da sigmoid'''\n",
    "    return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicação\n",
    "A fim de simplificar as explicações abaixo usaremos um mesmo modelo de rede neural em todos os exemplos apresentados. Nosso modelo terá duas entradas, 3 neurônios na camada escondida e 1 saída. Todos neurônios estão ligados a todos os neurônios da camada posterior a ele e não há recorrências, só é possível alimentar a rede da esquerda para a direita. No código, `camadas=[2,3,1]`.\n",
    "![exemplo](img/MLP231.png)\n",
    "[Fonte](https://www.opencadd.com.br/events/como-se-treina-uma-rede-neural/redes-neurais/)\n",
    "### Função construtor\n",
    "Assim que o objeto é criado, são inicializadas 4 características: quantidade de camadas, lista com quantidade de neurônios em cada camada, vieses e pesos. Todo neurônio tem um viés próprio, um valor real independente dos demais, que será ajustado durante o treino. Os pesos também são valores reais e independentes mas que relacionam um par de neurônios, representando a força da ligação entre eles dois, e também serão ajustados pelo treino. \n",
    "\n",
    "Treino neste contexto significa unicamente buscar pelos pesos e vieses que produzem o menor valor na função de custo.\n",
    "\n",
    "No código, os vieses de uma camada é guardada numa lista com tamanho igual a quantidade de neurônios. As listas com vieses de cada camada compõe também outra lista, formando uma array bidimensional. Temos então uma matriz neste formato: vieses = $[[v_{12}, v_{22}, v_{32}], [v_{13}]]$. Sendo $v_{ij}$ o viés i da camada j. Lembrando que a camada de entrada não tem neurônios nem vieses.\n",
    "\n",
    "Já os pesos entre duas camadas apenas configuram sozinhos uma matriz na qual o índice da coluna representa o neurônio de origem e o índice da linha os de chegada. Se dessa vez nomearmos cada neurônio da rede inteira com um valor único (de 1 a 6) e adotarmos a nomenclatura $p_{ij}$ para o peso que sai do neurônio i e chega em j temos a matriz de pesos relacionando a camada 1 e 2: pesos_12 = $[[p_{13},p_{23}],[p_{14},p_{24}],[p_{15},p_{25}]]$. Da camada 2 para 3: pesos23 = $[[p_{36}, p_{46}, p_{56}]]$. Faremos, claro, uma lista com estas duas matrizes. No código, `pesos = [pesos12, pesos23]`.\n",
    "\n",
    "OBS.: Nas listas do parágrafo acima, as entradas têm índices 1 e 2; os neurônios da camada oculta são 3, 4 e 5; o na saída é 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Função custo\n",
    "É geralmente usada para avaliar a rede neural. A função custo deve receber uma entrada e sua saída produzida pela rede, retornando assim um valor real que representa a diferença entre elas. A medida de distância escolhida foi a seguinte:\n",
    "$$D(a,b) = \\frac{1}{2}(y-g)^{2}$$\n",
    "sendo:\n",
    "* y = resposta da rede\n",
    "* g = resposta correta\n",
    "\n",
    "Na execução do backpropagation precisaremos de sua derivada, por isso também definimos-a no escopo da classe.\n",
    "$$\\dfrac{dD}{dy} = y-g$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função adiante\n",
    "Essa foi uma tentativa de tradução da palavra \"feedforward\", nome original do processo de cálculo da saída. Ela recebe uma amostra, que deve ser compatível com a entrada da rede, e retorna duas estruturas: a saída e a métrica definida no escopo da classe. A saída é uma lista na qual cada elemento representa um neurônio. A outra é o retorno da função `fcusto(a,b)`, cujo objetivo é dizer o quão distance `a` está de `b`.\n",
    "\n",
    "Entrando em detalhes, temos a lista `entrada` que é passada por parâmetro. A mesma será usada para calcular as entradas dos próximos neurônios, guardados em `prox_entrada`, e que por sua vez sobrescreverá as entradas iniciais na linha `entrada = prox_entrada.copy()`. Ao final do FOR, `entrada` conterá a  saída da rede.\n",
    "\n",
    "A forma como estão dispostos os pesos e vieses nos permite utilizarmos da operação _produto escalar_ para calcular o somatório ponderado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "Em meados da década de 1980 surgiu a descrição do algoritmo de treinamento backpropagation (Rumelhart, Hinton, & Williams 1986). O termo backpropagation surge do fato que o algoritmo se baseia na retropropagação dos erros para realizar os ajustes de pesos das camadas intermediárias. A maneira de calcular as derivadas parciais do erro de saída em relação a cada um dos pesos da rede é o que caracteriza o backpropagation. [Fonte](https://pt.wikipedia.org/wiki/Rede_neural_artificial)\n",
    "\n",
    "A função que imlementamos recebe um par (amostra, gabarito), calcula a saída e calcula todos os gradientes necessários para atualizar os pesos e vieses. \n",
    "\n",
    "O gradiente é calculado diferentemente para camada de saída e camadas ocultas:\n",
    "\n",
    "Para camada de saída: $$\\delta_{j}^{L}[n] = e_{j}^{L}[n]\\varphi'(z_{j}^{L}[n])$$\n",
    "sendo:\n",
    "* $e_{j}$ = diferença entre saída e gabarito\n",
    "* $\\varphi$  = função de ativação\n",
    "* $z_{j}[n]$ = valor do neurônio j antes de passar pela função de ativação\n",
    "\n",
    "\n",
    "Para camada oculta L: $$\\delta_{j}^{L}[n] = \\varphi'(z_{j}^{L}[n])\\sum_{k\\in C} \\delta_{k}^{L+1}[n] w_{jk}^{L}[n]$$\n",
    "sendo: \n",
    "* C = número de neurônios na camada seguinte à em análize\n",
    "* $\\varphi$  = função de ativação\n",
    "* $\\delta_{k}^{L+1}$ = gradiente de um neurônio k na camada seguinte\n",
    "* $w_{jk}^{L}[n]$ = peso que sai do neurônio j (camada L) e a vai à k (camada L+1)\n",
    "\n",
    "Baseado no livro \"Neural Networks - A Comprehensive Foundation\", de Simon Haykin.\n",
    "\n",
    "\n",
    "É importante notar que gradientes são valores associados aos neurônios e cada um teu o seu. Mas não os guardamos de forma intuitiva. Vejamos o nosso exemplo de rede [2,3,1]. Nele, nós temos 4 gradientes (1 pro neurônio de saída e 3 pra os da camada oculta), mas inicialmente criamos uma estrutura com o formato de `self.pesos` com todos valores 0, isto na linha `grad_p = [np.zeros(p.shape) for p in self.pesos]`. Nossa intenção é guardar o gradiente usado para calcular o novo peso na mesma posição em ambas listas. O valor `grad_p[i][j]` será usado para calcular o novo peso em `self.pesos[i][j]`. Para isso ser possível nos vemos obrigados a gravar o mesmo gradiente em várias posições de `grad_p`, ou seja, estamos sacrificando memória em troca de maior velocidade na execução.\n",
    "\n",
    "Na verdade o que estamos salvando em `grad_p` é o gradiente já multiplicado pelo valor do neurônio ao qual pertence, também por praticidade. Vemos anteiormente que os pesos da camada 1 para 2 estão na lista `pesos_12` = $[[p_{13},p_{23}],[p_{14},p_{24}],[p_{15},p_{25}]]$, os gradientes estarão dispostos assim: `grad_p12` = $[[g_{3}y_{3},g_{3}y_{3}],[g_{4}y_{4},g_{4}y_{4}],[g_{5}y_{5},g_{5}y_{5}]]$.\n",
    "\n",
    "Sendo assim, poderemos atualizar um peso pela fórmula: $$w_{ij}[n+1] = w_{ij}[n] - \\eta\\delta_{j}y_{j}$$\n",
    "onde: \n",
    "* $\\eta$ = taxa de aprendizagem\n",
    "* $w_{ij}$ = peso saindo do neurônio i e chega ao j\n",
    "* $y_{j}$ = valor do neurônio j depois de passar pela função de ativação\n",
    "\n",
    "Obviamente a mesma lógica é usada com os vieses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Função Atualiza Lote\n",
    "Nela são atualizados os pesos e vieses. Ao receber um conjunto de amostras e a taxa de aprendizagem, realiza os seguintes passos:\n",
    "* Cria as estruturas de listas com mesmo formato das variáveis `self.pesos` e `self.vieses`, necessárias para armazenar os gradientes que serão calculados\n",
    "* Para cada amosta passada como parâmetro, usa-a para executar o bakcpropagation e guarda os gradientes retornados\n",
    "* Os gradientes provenientes de cada amostra são acumulados (i.e. somados) nas variáveis do item 1\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
